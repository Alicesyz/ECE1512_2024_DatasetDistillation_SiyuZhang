# -*- coding: utf-8 -*-
"""TASK1_D_E_MNIST_GAUSSIAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K7qlEjqwY6Gx52YL5pixjQMijdiJkOhX

condensed images are initialized with Gaussian noise --- MNIST
"""

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets, transforms
from networks import ConvNet
import random
from ptflops import get_model_complexity_info
from PIL import Image
from torch.optim.lr_scheduler import CosineAnnealingLR
import os
from torchvision.utils import save_image

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the attention matching function with synthetic data initialized from real images
def run_attention_matching_mnist(
    num_weight_initializations=100,
    model_update_steps=50,
    lr_synthetic=0.1,
    synthetic_update_steps=1,
    lr_model=0.01,
    task_balance_lambda=0.01,
    num_epochs=20,
    images_per_class=10,
    minibatch_size=256
):
    # Set up MNIST data loader
    transform = transforms.Compose([transforms.ToTensor()])
    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

    train_loader = DataLoader(mnist_train, batch_size=minibatch_size, shuffle=True)
    test_loader = DataLoader(mnist_test, batch_size=minibatch_size, shuffle=False)

    # Initialize synthetic data for 10 images per class (100 total images for MNIST)
    num_classes = 10
    synthetic_data = [
        torch.randn((images_per_class, 1, 28, 28), requires_grad=True, device=device)
        for _ in range(num_classes)
    ]
    synthetic_labels = torch.tensor([i for i in range(num_classes) for _ in range(images_per_class)], device=device)

    # Start Attention Matching process
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        epoch_loss = 0  # Track loss for each epoch

        for weight_init_iter in range(num_weight_initializations):
            # Initialize the ConvNet model for each reinitialization
            model = ConvNet(
                channel=1, num_classes=10, net_width=128, net_depth=3,
                net_act='relu', net_norm='batchnorm', net_pooling='avgpooling', im_size=(28, 28)
            ).to(device)

            model_optimizer = optim.SGD(model.parameters(), lr=lr_model, momentum=0.9)

            # Model training on synthetic data for `model_update_steps` iterations
            for update_step in range(model_update_steps):
                synthetic_inputs = torch.cat([sd.clone().detach().requires_grad_(True) for sd in synthetic_data]).to(device)

                model_output = model(synthetic_inputs)
                model_loss = nn.CrossEntropyLoss()(model_output, synthetic_labels)

                model_optimizer.zero_grad()
                model_loss.backward()
                model_optimizer.step()
                epoch_loss += model_loss.item()

            # Update synthetic data with attention matching
            for synthetic_step in range(synthetic_update_steps):
                for class_idx, synthetic_class_data in enumerate(synthetic_data):
                    synthetic_class_data = synthetic_class_data.clone().detach().requires_grad_(True)
                    synthetic_optimizer = optim.SGD([synthetic_class_data], lr=lr_synthetic)

                    real_images, real_labels = next(iter(train_loader))
                    real_images, real_labels = real_images.to(device), real_labels.to(device)

                    model.eval()
                    synthetic_output = model(synthetic_class_data)
                    real_output = model(real_images)

                    attention_loss = task_balance_lambda * ((synthetic_output - real_output[:images_per_class].detach()) ** 2).mean()

                    synthetic_optimizer.zero_grad()
                    attention_loss.backward()
                    synthetic_optimizer.step()

                    synthetic_data[class_idx] = synthetic_class_data.detach().requires_grad_(True)

            #print(f"Reinitialization {weight_init_iter+1}/{num_weight_initializations} - Attention Loss: {attention_loss.item()}")

        print(f"Epoch {epoch+1} Completed - Average Loss: {epoch_loss / (num_weight_initializations * model_update_steps):.4f}")

    # Visualize the condensed synthetic images
    save_synthetic_images(synthetic_data)

    # Evaluate the model on the test set
    evaluate_model(model, test_loader)

def save_synthetic_images(synthetic_data, save_dir="synthetic_images"):
    os.makedirs(save_dir, exist_ok=True)  # Create directory to save images

    num_classes = len(synthetic_data)
    images_per_class = synthetic_data[0].shape[0]  # Number of images per class

    for class_idx, class_data in enumerate(synthetic_data):
        for img_idx in range(images_per_class):
            # Extract each image, resize to 28x28 if necessary, and save it
            image = class_data[img_idx].detach().cpu()

            # Save each image with a specific filename
            filename = f"{save_dir}/class_{class_idx}_image_{img_idx + 1}.png"
            save_image(image, filename)
            #print(f"Saved {filename}")

def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    test_accuracy = correct / total
    print(f"Test Accuracy: {test_accuracy:.10f}")

run_attention_matching_mnist()

"""////////// E"""

!pip install ptflops
!pip install torchprofile

"""// with training time"""

import time  # Import time module for timing


# Load Synthetic MNIST Dataset from saved images
def load_synthetic_dataset(synthetic_dir="/content/synthetic_images_noise", num_classes=10, images_per_class=10):
    synthetic_images = []
    synthetic_labels = []

    for class_idx in range(num_classes):
        for img_idx in range(1, images_per_class + 1):
            img_path = os.path.join(synthetic_dir, f"class_{class_idx}_image_{img_idx}.png")

            # Load the image in grayscale mode to ensure it has 1 channel
            image = Image.open(img_path).convert('L')  # Convert to grayscale
            image = transforms.ToTensor()(image)       # Convert to tensor
            synthetic_images.append(image)
            synthetic_labels.append(class_idx)

    # Convert lists to tensors and ensure correct data types
    # Ensure FloatTensor for images
    synthetic_images = torch.stack(synthetic_images).float().to(device)
    # Ensure LongTensor for labels
    synthetic_labels = torch.tensor(synthetic_labels, dtype=torch.long, device=device)

    # Create a TensorDataset with the images and labels
    synthetic_dataset = TensorDataset(synthetic_images, synthetic_labels)
    return synthetic_dataset

# Train model on synthetic data and measure training time
def train_on_synthetic_data(synthetic_loader, num_epochs=20, lr=0.01):
    model = ConvNet(channel=1, num_classes=10, net_width=128, net_depth=3, net_act='relu',
                    net_norm='batchnorm', net_pooling='avgpooling', im_size=(28, 28)).to(device)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
    criterion = F.cross_entropy

    # Start timing the training
    start_time = time.time()

    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        for images, labels in synthetic_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            # Directly use model(images) without indexing
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

            # Calculate correct predictions for accuracy
            _, predicted = outputs.max(1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        epoch_accuracy = 100.0 * correct / total  # Calculate accuracy for the epoch
        scheduler.step()
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(synthetic_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%")

    total_training_time = time.time() - start_time  # Calculate total training time
    print(f"Total Training Time: {total_training_time:.2f} seconds")

    return model, total_training_time

# Model evaluation function on real MNIST test set
def evaluate_on_test_set(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)


            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total
    print(f"Test Accuracy on Real MNIST Test Set: {accuracy:.4f}")
    return accuracy

# Calculate FLOPs and Parameters
def calculate_flops(model, input_resolution=(1, 28, 28)):
    flops, params = get_model_complexity_info(model, input_resolution, as_strings=True, print_per_layer_stat=False)
    print(f"FLOPs: {flops}")

# Main function to run the training and evaluation
def run_experiment():
    # Load synthetic dataset
    synthetic_dataset = load_synthetic_dataset()
    synthetic_loader = DataLoader(synthetic_dataset, batch_size=10, shuffle=True)

    # Load MNIST test dataset
    transform = transforms.Compose([transforms.ToTensor()])
    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    test_loader = DataLoader(mnist_test, batch_size=256, shuffle=False)

    # Train the model on synthetic data and measure training time
    model, total_training_time = train_on_synthetic_data(synthetic_loader)

    # Evaluate on real MNIST test set
    evaluate_on_test_set(model, test_loader)

    # Calculate FLOPs and Parameters
    calculate_flops(model)

"""//real image"""

# Run the experiment
run_experiment()

"""//gaussian noise version"""

# Run the experiment
run_experiment()
# -*- coding: utf-8 -*-
"""TASK2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VsTQKzB3iehT5bhNOhUBmaP3oKOoszbt
"""

!pip install ptflops

import os
import time
import torch
import torch.optim as optim
from torch import nn
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
from torchvision.utils import save_image
from networks import ConvNet
from ptflops import get_model_complexity_info
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#define transformation for MNIST images
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor(),
])

#doad the original MNIST dataset
mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)

#select initialize synthetic dataset from real images
def create_synthetic_mnist(images_per_class=10):
    synthetic_data = []
    labels = []

    #randomly select from each class
    for label in range(10):
        class_indices = [i for i, target in enumerate(mnist_data.targets) if target == label]
        selected_indices = random.sample(class_indices, images_per_class)

        for index in selected_indices:
            img, _ = mnist_data[index]
            synthetic_data.append(img)
            labels.append(label)

    synthetic_data = torch.stack(synthetic_data).to(device)
    labels = torch.tensor(labels).to(device)
    return synthetic_data, labels

#create synthetic dataset
synthetic_data, synthetic_labels = create_synthetic_mnist()

#parameters for PAD optimization
num_weight_initializations = 100
num_iterations = 10
lr_synthetic = 0.1
synthetic_update_steps = 1
lr_model = 0.01
model_update_steps = 50

#using PAD to train the selected synthetic images
def pad_optimization(synthetic_data, synthetic_labels, model, num_weight_initializations, num_iterations, lr_synthetic, synthetic_update_steps, lr_model, model_update_steps):
    criterion = nn.CrossEntropyLoss()

    for init in range(num_weight_initializations):
        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)
        optimizer_model = optim.SGD(model.parameters(), lr=lr_model, momentum=0.9)

        for iteration in range(num_iterations):
            synthetic_optimizer = optim.SGD([synthetic_data.requires_grad_(True)], lr=lr_synthetic)

            #training on synthetic data
            for step in range(model_update_steps):
                outputs = model(synthetic_data)
                loss = criterion(outputs, synthetic_labels)
                optimizer_model.zero_grad()
                loss.backward()
                optimizer_model.step()

            #update synthetic images
            for step in range(synthetic_update_steps):
                synthetic_optimizer.zero_grad()
                outputs = model(synthetic_data)
                loss = criterion(outputs, synthetic_labels)
                loss.backward()
                synthetic_optimizer.step()


    #save the synthetic images after PAD processing
    synthetic_save_dir = "synthetic_mnist_pad"
    os.makedirs(synthetic_save_dir, exist_ok=True)

    #save into pt
    torch.save((synthetic_data, synthetic_labels), os.path.join(synthetic_save_dir, "optimized_synthetic_data.pt"))

    #save each image with name
    for class_idx in range(10):
        class_images = synthetic_data[synthetic_labels == class_idx]
        for img_idx, img in enumerate(class_images):
            save_path = os.path.join(synthetic_save_dir, f"class_{class_idx}_image_{img_idx + 1}.png")
            save_image(img.cpu(), save_path)

##train the sythetic images from the convnet-3 model as before

#initialize model and  synthetic data
synthetic_data, synthetic_labels = create_synthetic_mnist()
synthetic_data, synthetic_labels = synthetic_data.to(device), synthetic_labels.to(device)
model_for_pad = ConvNet(channel=1, num_classes=10, net_width=256, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='avgpooling').to(device)

pad_optimization(synthetic_data, synthetic_labels, model_for_pad, num_weight_initializations, num_iterations, lr_synthetic, synthetic_update_steps, lr_model, model_update_steps)

#load synthetic data
class SyntheticDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image, label = self.data[idx], self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

synthetic_dataset = SyntheticDataset(synthetic_data, synthetic_labels)
synthetic_loader = DataLoader(synthetic_dataset, batch_size=10, shuffle=True)

#load original MNIST test dataset
mnist_test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)
test_loader = DataLoader(mnist_test_dataset, batch_size=256, shuffle=False)

#calculate FLOPs using ptflops
def calculate_flops(model, input_size=(1, 28, 28)):
    flops, params = get_model_complexity_info(model, input_size, as_strings=True, print_per_layer_stat=False)
    print(f"FLOPs: {flops}, Params: {params}")
    return flops, params

#train ConvNet-3 model
def train_and_evaluate(model, train_loader, test_loader, epochs=20, initial_lr=0.01):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    #add the training time
    start_time = time.time()

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * labels.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        scheduler.step()
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / total:.4f}, Accuracy: {100.0 * correct / total:.2f}%")

    #add the training time
    training_time = time.time() - start_time
    print(f"Total Training Time: {training_time:.2f} seconds")

    #evaluate on test data
    model.eval()
    correct = 0
    total = 0
    test_loss = 0.0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            test_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = 100 * correct / total
    average_test_loss = test_loss / total
    print(f"\nTest Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%")

    #calculate FLOPs
    flops, params = calculate_flops(model, input_size=(1, 28, 28))

print("\nTraining ConvNet-3 on PAD-optimized synthetic dataset")
final_model = ConvNet(channel=1, num_classes=10, net_width=128, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='avgpooling')
train_and_evaluate(final_model, synthetic_loader, test_loader, epochs=20, initial_lr=0.01)

"""////visualize the images"""

import matplotlib.pyplot as plt
import torch

synthetic_data, synthetic_labels = torch.load('/content/synthetic_mnist_pad/optimized_synthetic_data.pt')

num_classes = 10
images_per_class = 10

fig, axs = plt.subplots(num_classes, images_per_class + 1, figsize=(15, 15))
fig.subplots_adjust(hspace=0.5)

for class_idx in range(num_classes):
    # Add the class label on the left side
    axs[class_idx, 0].text(0.5, 0.5, f"Class {class_idx}", ha='center', va='center', fontsize=12)
    axs[class_idx, 0].axis('off')

    class_images = synthetic_data[synthetic_labels == class_idx][:images_per_class]

    for img_idx, img in enumerate(class_images):
        img_np = img.cpu().detach().squeeze().numpy()

        #display images
        axs[class_idx, img_idx + 1].imshow(img_np, cmap='gray')
        axs[class_idx, img_idx + 1].axis('off')
plt.show()

!pip install ptflops

import os
import time
import torch
import torch.optim as optim
from torch import nn
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
from torchvision.utils import save_image
from networks import ConvNet
from ptflops import get_model_complexity_info
import random

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define transformation for MNIST images
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor(),
])

# Load the original MNIST dataset
mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)

# Initialize model for PAD optimization
model_for_pad = ConvNet(channel=1, num_classes=10, net_width=128, net_depth=3,
                        net_act='relu', net_norm='batchnorm', net_pooling='avgpooling').to(device)
class SyntheticDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image, label = self.data[idx], self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label
# Step 1: Initialize with EL2N Scoring
def calculate_el2n_score(model, image, label):
    image = image.unsqueeze(0).to(device)
    label = torch.tensor([label], device=device)
    image.requires_grad = True

    output = model(image)
    criterion = nn.CrossEntropyLoss()
    loss = criterion(output, label)

    loss.backward()
    grad = image.grad

    el2n_score = grad.norm().item()
    return el2n_score

def create_synthetic_mnist_with_el2n(images_per_class=100, compression_ratio=10):
    synthetic_data = []
    labels = []
    el2n_scores = {}

    scoring_model = ConvNet(channel=1, num_classes=10, net_width=128, net_depth=3,
                            net_act='relu', net_norm='batchnorm', net_pooling='avgpooling').to(device)
    scoring_model.eval()

    target_samples_per_class = images_per_class // compression_ratio

    for label in range(10):
        class_indices = [i for i, target in enumerate(mnist_data.targets) if target == label]
        selected_indices = random.sample(class_indices, images_per_class)

        el2n_scores[label] = []
        for index in selected_indices:
            img, _ = mnist_data[index]
            el2n_score = calculate_el2n_score(scoring_model, img, label)
            el2n_scores[label].append((img, label, el2n_score))

        sorted_images = sorted(el2n_scores[label], key=lambda x: x[2], reverse=True)
        top_images = sorted_images[:target_samples_per_class]

        for img, label, score in top_images:
            synthetic_data.append(img)
            labels.append(label)

    synthetic_data = torch.stack(synthetic_data).to(device)
    labels = torch.tensor(labels).to(device)
    return synthetic_data, labels

# Create synthetic dataset using EL2N scoring
synthetic_data, synthetic_labels = create_synthetic_mnist_with_el2n(images_per_class=100, compression_ratio=10)

# Step 2: Information Embedding (PAD Optimization)
num_weight_initializations = 100
num_iterations = 10
lr_synthetic = 0.1
synthetic_update_steps = 1
lr_model = 0.01
model_update_steps = 50

# Step 2: Information Embedding (PAD Optimization)
def pad_optimization_with_deep_layers(synthetic_data, synthetic_labels, model,
                                      num_weight_initializations, num_iterations,
                                      lr_synthetic, synthetic_update_steps,
                                      lr_model, model_update_steps):
    criterion = nn.CrossEntropyLoss()

    # Prepare DataLoader for batch processing
    synthetic_dataset = SyntheticDataset(synthetic_data, synthetic_labels)
    synthetic_loader = DataLoader(synthetic_dataset, batch_size=10, shuffle=True)

    # Target deep layer for PAD
    deep_layer_name = 'features'  # Assuming 'features' contains the last block of conv layers
    model_for_loss = getattr(model, deep_layer_name)

    for init in range(num_weight_initializations):
        # Reinitialize model weights for each run
        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)
        optimizer_model = optim.SGD(model.parameters(), lr=lr_model, momentum=0.9)

        for iteration in range(num_iterations):
            # Optimizer for synthetic images
            synthetic_optimizer = optim.SGD([synthetic_data.requires_grad_(True)], lr=lr_synthetic)

            # Model training on synthetic data in batches
            for images, labels in synthetic_loader:
                images, labels = images.to(device), labels.to(device)

                # Forward pass through the model to the specified deep layer
                outputs = model_for_loss(images)
                loss = criterion(outputs, labels)

                optimizer_model.zero_grad()
                loss.backward()
                optimizer_model.step()

            # Update synthetic images based on deep layer features
            for images, labels in synthetic_loader:
                images = images.requires_grad_(True).to(device)
                labels = labels.to(device)

                synthetic_optimizer.zero_grad()
                outputs = model_for_loss(images)
                loss = criterion(outputs, labels)
                loss.backward()
                synthetic_optimizer.step()

            print(f"Initialization {init + 1}, Iteration {iteration + 1}, Loss: {loss.item()}")

    # Save optimized synthetic images
    synthetic_save_dir = "synthetic_mnist_pad"
    os.makedirs(synthetic_save_dir, exist_ok=True)
    torch.save((synthetic_data, synthetic_labels), os.path.join(synthetic_save_dir, "optimized_synthetic_data.pt"))

    for class_idx in range(10):
        class_images = synthetic_data[synthetic_labels == class_idx]
        for img_idx, img in enumerate(class_images):
            save_path = os.path.join(synthetic_save_dir, f"class_{class_idx}_image_{img_idx + 1}.png")
            save_image(img.cpu(), save_path)

# Call PAD optimization
pad_optimization_with_deep_layers(synthetic_data, synthetic_labels, model_for_pad,
                                  num_weight_initializations, num_iterations,
                                  lr_synthetic, synthetic_update_steps,
                                  lr_model, model_update_steps)



synthetic_dataset = SyntheticDataset(synthetic_data, synthetic_labels)
synthetic_loader = DataLoader(synthetic_dataset, batch_size=10, shuffle=True)

mnist_test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)
test_loader = DataLoader(mnist_test_dataset, batch_size=256, shuffle=False)

print("\nTraining ConvNet-3 on PAD-optimized synthetic dataset")
final_model = ConvNet(channel=1, num_classes=10, net_width=128, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='avgpooling')
train_and_evaluate(final_model, synthetic_loader, test_loader, epochs=20, initial_lr=0.01)
# -*- coding: utf-8 -*-
"""TASK1_A_ORIGINAL_MHIST

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h2eRliQbCUb8hIhFbIB7b5XtKGs1KUYy
"""

import torch
print(torch.cuda.is_available())

!pip install ptflops
!pip install torchprofile

import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from networks import ConvNet
from PIL import Image
import pandas as pd
from ptflops import get_model_complexity_info
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#import drive for colab
from google.colab import drive
drive.mount('/content/gdrive')

#mhist dataset path
data_path = '/content/gdrive/MyDrive/submission_files/mhist_dataset/'
annotations_file = '/content/gdrive/MyDrive/submission_files/mhist_dataset/annotations.csv'
img_dir = '/content/gdrive/MyDrive/submission_files/mhist_dataset/images'

#define mhist dataset class
class MHISTDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.label_map = {"HP": 0, "SSA": 1}

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = Image.open(img_path).convert("RGB")
        label = self.img_labels.iloc[idx, 1]
        label = self.label_map[label]  # convert label from string to integer

        if self.transform:
            image = self.transform(image)
        return image, label

#define function to load mhist dataset
def get_dataset(data_path):
    channel = 3
    im_size = (128, 128)
    num_classes = 2
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]

    #define transformations
    transform = transforms.Compose([
        transforms.Resize(im_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])

    #load mhist dataset
    dst_train = MHISTDataset(annotations_file, img_dir, transform=transform)
    dst_test = MHISTDataset(annotations_file, img_dir, transform=transform)
    test_loader = DataLoader(dst_test, batch_size=128, shuffle=False)
    return channel, im_size, num_classes, dst_train, test_loader

#load mhist dataset
channel, im_size, num_classes, dst_train, test_loader = get_dataset(data_path)
train_loader = DataLoader(dst_train, batch_size=128, shuffle=True)

#define convnet-7 model for mhist
model = ConvNet(
    channel=channel,
    num_classes=num_classes,
    net_width=128,
    net_depth=7,
    net_act='relu',
    net_norm='instancenorm',
    net_pooling='avgpooling',
    im_size=im_size
).to(device)

#initialize optimizer, scheduler, and loss function
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
criterion = torch.nn.CrossEntropyLoss().to(device)

#train model with accuracy calculation for each epoch
def train_model(model, train_loader, optimizer, scheduler, num_epochs=20):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            #accumulate loss
            running_loss += loss.item() * images.size(0)

            #calculate accuracy
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        #calculate average loss and accuracy for the epoch
        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_accuracy = 100 * correct / total

        scheduler.step()
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")

#test model function
def test_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in test_loader:
            images, labels = batch[0].to(device), batch[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f"Test Accuracy: {accuracy:.2f}%")
    return accuracy

#train the model
train_model(model, train_loader, optimizer, scheduler, num_epochs=20)

#test the model
accuracy = test_model(model, test_loader)

#calculate flops and parameters
flops, params = get_model_complexity_info(model, (channel, *im_size), as_strings=True, print_per_layer_stat=False)
print(f"FLOPs: {flops}, Parameters: {params}")

from ptflops import get_model_complexity_info

flops, params = get_model_complexity_info(model, (3, 128, 128), as_strings=False, print_per_layer_stat=False)
print(f"FLOPs: {flops:.2e}")